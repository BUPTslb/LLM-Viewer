{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hardwares.hardware_params import hardware_params\n",
    "from model_analyzer import ModelAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Llama-2-13b-hf\"\n",
    "hardware=\"nvidia_A6000\"\n",
    "analyzer=ModelAnalyzer(model_id,hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# styles = plt.cm.Set2(range(3))\n",
    "# plt.rcParams['axes.prop_cycle'] = plt.cycler('color', styles)\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "bar_width = 0.7\n",
    "\n",
    "for step in ['decode','prefill']:\n",
    "    plt.subplot(2, 1, 1 if step=='decode' else 2)\n",
    "    weights=[]\n",
    "    kvs=[]\n",
    "    tmp_acts=[]\n",
    "    categories=[]\n",
    "    annotation_xs=[]\n",
    "    annotation_texts=[]\n",
    "    xs=[]\n",
    "    x_st=0\n",
    "    for batchsize,seqlen in [(1,1024),(1,4096),(16,1024)]:\n",
    "        FP16_sum=0\n",
    "        annotation_xs.append(x_st)\n",
    "        annotation_texts.append(f\"{step}\\nbatchsize={batchsize}\\n seqlen={seqlen}\")\n",
    "        for w,a,kv,quantization in [(16,16,16,\"FP16\"),(4,16,16,\"W4\"),(4,16,4,\"W4KV4\"),(4,4,4,\"W4A4\")]:\n",
    "            print(f\"batchsize={batchsize}, seqlen={seqlen}, w={w}, a={a}, kv={kv}\")\n",
    "            result=analyzer.analyze(batchsize,seqlen,w,a,kv)\n",
    "            weights.append(result[\"total_results\"][step][\"memory_consumption_weight\"])\n",
    "            kvs.append(result[\"total_results\"][step][\"memory_consumption_kv_cache\"])\n",
    "            tmp_acts.append(result[\"total_results\"][step][\"memory_consumption_tmp_act\"])\n",
    "            xs.append(x_st)\n",
    "            x_st+=1\n",
    "            # if quantization==\"W4\":\n",
    "            #     categories.append(f\"{quantization}\\nbs={batchsize}\\nlen={seqlen}\")\n",
    "            # else:\n",
    "            categories.append(f\"{quantization}\")\n",
    "            if quantization==\"FP16\":\n",
    "                FP16_sum=weights[-1]+kvs[-1]+tmp_acts[-1]\n",
    "            weights[-1]/=FP16_sum\n",
    "            kvs[-1]/=FP16_sum\n",
    "            tmp_acts[-1]/=FP16_sum\n",
    "        x_st+=1\n",
    "    plt.bar(xs, weights, bar_width, label='Weight')\n",
    "    plt.bar(xs, kvs, bar_width, bottom=weights, label='KV cache')\n",
    "    plt.bar(xs, tmp_acts, bar_width, bottom=np.array(kvs) + np.array(weights), label='Tmp Act')\n",
    "\n",
    "    for ann_x,ann_text in zip(annotation_xs,annotation_texts):\n",
    "        plt.annotate(ann_text, (ann_x+2.2, 0.75), ha='center')\n",
    "    plt.ylabel('Relative Memory Consumption', fontsize=9)\n",
    "    plt.xticks(xs, categories, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "plt.savefig(\"../output/quantization_memory_consumption.pdf\",bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "houmo_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
