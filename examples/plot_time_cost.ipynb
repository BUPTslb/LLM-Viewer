{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hardwares.hardware_params import hardware_params\n",
    "from model_analyzer import ModelAnalyzer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Llama-2-13b-hf\"\n",
    "hardware=\"nvidia_A6000\"\n",
    "analyzer=ModelAnalyzer(model_id,hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(5, 5))\n",
    "bar_width = 0.7\n",
    "\n",
    "for step in ['decode','prefill']:\n",
    "    plt.subplot(2, 1, 1 if step=='decode' else 2)\n",
    "    ax=plt.gca()  # twin axis\n",
    "    # twin axis\n",
    "    ax2 = plt.twinx()\n",
    "\n",
    "    for w,a,kv,quantization in [(16,16,16,\"FP16\"),(4,16,16,\"W4\"),(4,16,4,\"W4KV4\"),(4,4,4,\"W4A4\")]:\n",
    "        inference_times=[]\n",
    "        weight_kv_memory_access=[]\n",
    "        xs=[]\n",
    "        batchsize=1\n",
    "        \n",
    "        for seqlen_p2 in range(8,13):\n",
    "            seqlen=2**seqlen_p2\n",
    "            result=analyzer.analyze(seqlen,batchsize,w,a,kv)\n",
    "            inference_times.append(result[\"total_results\"][step][\"inference_time\"])\n",
    "            weight_kv_memory_access.append(result[\"total_results\"][step][\"load_weight\"]+result[\"total_results\"][step][\"load_kv_cache\"]+result[\"total_results\"][step][\"store_kv_cache\"])\n",
    "            xs.append(seqlen)\n",
    "        \n",
    "        ax.plot(xs, inference_times, label=f\"{quantization} time cost\")\n",
    "        ax2.plot(xs, weight_kv_memory_access,'--', label=f\"{quantization} W+KV memory access\")\n",
    "    plt.xscale('log',base=2)\n",
    "    # plt.ylabel('Relative Memory Consumption', fontsize=9)\n",
    "    # ax.legend(loc='upper left', fontsize=9)\n",
    "    # ax2.legend(loc='upper right', fontsize=9)\n",
    "    # merge legend\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines + lines2, labels + labels2, loc='upper right', fontsize=9)\n",
    "\n",
    "    \n",
    "    plt.xlabel('Sequence Length', fontsize=9)\n",
    "    ax.set_ylabel('Time Cost (s)', fontsize=9)\n",
    "    ax2.set_ylabel('W+KV Memory Access', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "# plt.savefig(\"../output/quantization_memory_consumption.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(5, 2))\n",
    "for wbit in [16,8,4,2,1]:\n",
    "    batchsizes=range(1,64)\n",
    "    ys=[]\n",
    "    for batchsize in batchsizes:\n",
    "        result=analyzer.analyze(1024,batchsize,wbit,16,16)\n",
    "        ys.append(result[\"total_results\"][\"decode\"][\"inference_time\"])\n",
    "    plt.plot(batchsizes,ys,label=f\"W{wbit}\" if wbit!=16 else \"FP16\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Inference Time (s)\")\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.savefig(\"../output/quantization_memory_access_batch.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(5, 2))\n",
    "for wbit in [16,8,4,2,1]:\n",
    "    seqlens=range(4, 11)\n",
    "    seqlens=range(16,1024)\n",
    "    ys=[]\n",
    "    for seqlen in seqlens:\n",
    "        # seqlen=2**seqlen\n",
    "        result=analyzer.analyze(seqlen,1,wbit,16,16)\n",
    "        ys.append(result[\"total_results\"][\"prefill\"][\"inference_time\"])\n",
    "    plt.plot(seqlens,ys,label=f\"W{wbit}\" if wbit!=16 else \"FP16\")\n",
    "plt.legend()\n",
    "plt.xscale('log',base=2)\n",
    "plt.ylabel(\"Inference Time (s)\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.savefig(\"../output/quantization_memory_access_seq_len.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "houmo_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
