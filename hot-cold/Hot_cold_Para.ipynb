{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31695ca4",
   "metadata": {},
   "source": [
    "# 冷热神经元分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62197457",
   "metadata": {},
   "source": [
    "数值分析代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667e7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功！\n",
      "模型总层数: 26\n",
      "\n",
      "=== 处理数据集: wikitext (split: test) ===\n",
      "警告: 数据集只有 321 个有效样本\n",
      "使用 321 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 321/321 [00:11<00:00, 28.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果保存为: google_gemma_2_2b_it/wikitext_activation_results.json\n",
      "\n",
      "=== 处理数据集: gsm8k (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:16<00:00, 30.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果保存为: google_gemma_2_2b_it/gsm8k_activation_results.json\n",
      "\n",
      "=== 处理数据集: cc_news (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:15<00:00, 32.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果保存为: google_gemma_2_2b_it/cc_news_activation_results.json\n",
      "\n",
      "=== 处理数据集: squad (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:14<00:00, 34.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果保存为: google_gemma_2_2b_it/squad_activation_results.json\n",
      "\n",
      "=== 处理数据集: cnn_dailymail (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:15<00:00, 32.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果保存为: google_gemma_2_2b_it/cnn_dailymail_activation_results.json\n",
      "\n",
      "所有数据集处理完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm  # 进度条\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Enable expandable segments to avoid fragmentation (add this before imports if needed)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 配置\n",
    "# model_name =    \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# model_name =    \"meta-llama/Llama-3.2-1B-Instruct\" # Llama3.2-1B (Instruct版本)\n",
    "# model_name =    \"Qwen/Qwen1.5-1.8B-Chat\"          # Qwen1.5-1.8B (Chat版本)\n",
    "# model_name =    \"google/gemma-2-2b-it\"             # Gemma2-2B (Instruct版本)\n",
    "# model_name =    \"microsoft/phi-2\"                  # Phi-2-2.7B (基础版本，无Instruct)\n",
    "# model_name =    \"microsoft/Phi-3.5-mini-instruct\"  # Phi-3.5-Mini-Instruct (3.8B Mini版本)\n",
    "# model_name =    \"Qwen/Qwen1.5-4B-Chat\"            # Qwen1.5 4B (Chat版本)\n",
    "# model_name =    \"THUDM/chatglm2-6b\"                # ChatGLM2 6B\n",
    "# model_name =    \"facebook/opt-6.7b\"                # OPT6.7B (基础版本)\n",
    "# model_name =    \"mistralai/Mistral-7B-Instruct-v0.1\" # Mistral-7B (Instruct版本)\n",
    "# model_name =    \"Qwen/Qwen2-7B-Instruct\"          # Qwen2-7B (Instruct版本)\n",
    "# model_name =    \"meta-llama/Meta-Llama-3-8B-Instruct\" # LLaMA3-8B (Instruct版本)\n",
    "num_samples = 500  # 每个数据集的样本数量，可调整为1000+\n",
    "max_length = 128   # 最大序列长度，控制内存\n",
    "activation_threshold = 1e-3  # 激活阈值（用于判断是否激活）\n",
    "hot_freq_threshold = 0.3     # 热神经元频率阈值（e.g., 激活频率 > 50%视为热，可调整）\n",
    "\n",
    "# 数据集列表：(dataset_name, config, split, text_column)\n",
    "# text_column 是提取文本的键；如果需要组合多个列，在代码中处理\n",
    "# 更新：gsm8k的config从None改为\"main\"\n",
    "datasets_to_use = [\n",
    "    (\"wikitext\", \"wikitext-2-raw-v1\", \"test\", \"text\"),  # 文本数据集\n",
    "    (\"gsm8k\", \"main\", \"train\", \"question\"),  # 数学问题（使用'main'配置）\n",
    "    (\"cc_news\", None, \"train\", \"text\"),  # 新闻文本（bookcorpus的替代）\n",
    "    (\"squad\", None, \"train\", \"question\"),  # 问答（将组合question + context）\n",
    "    (\"cnn_dailymail\", \"3.0.0\", \"train\", \"article\")  # 新闻文章\n",
    "]\n",
    "\n",
    "# 加载模型和tokenizer（只加载一次）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model.eval()  # 设置为评估模式\n",
    "    print(\"模型加载成功！\")\n",
    "    # Release any initial cached memory after loading\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"加载模型失败：{e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 定义钩子函数（移到钩子注册之前）\n",
    "handles = []\n",
    "def hook_fn(layer_idx):\n",
    "    def fn(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            activations_per_layer[layer_idx].append(output[0].detach().cpu())\n",
    "        else:\n",
    "            activations_per_layer[layer_idx].append(output.detach().cpu())\n",
    "    return fn\n",
    "\n",
    "# 获取层数和注册钩子（添加模型特定处理）\n",
    "if \"qwen\" in model_name.lower() or \"llama\" in model_name.lower() or \"gemma\" in model_name.lower() or \"mistral\" in model_name.lower() or \"phi\" in model_name.lower():\n",
    "    # 标准：model.model.layers[i].mlp\n",
    "    num_layers = len(model.model.layers)\n",
    "    for i in range(num_layers):\n",
    "        hook = model.model.layers[i].mlp.register_forward_hook(hook_fn(i))\n",
    "        handles.append(hook)\n",
    "elif \"opt\" in model_name.lower():\n",
    "    # OPT：model.decoder.layers[i].fc2 (捕获FFN输出)\n",
    "    num_layers = len(model.decoder.layers)\n",
    "    for i in range(num_layers):\n",
    "        hook = model.decoder.layers[i].fc2.register_forward_hook(hook_fn(i))\n",
    "        handles.append(hook)\n",
    "elif \"chatglm\" in model_name.lower():\n",
    "    # ChatGLM：model.transformer.encoder.layers[i].mlp\n",
    "    num_layers = len(model.transformer.encoder.layers)\n",
    "    for i in range(num_layers):\n",
    "        hook = model.transformer.encoder.layers[i].mlp.register_forward_hook(hook_fn(i))\n",
    "        handles.append(hook)\n",
    "else:\n",
    "    raise ValueError(f\"不支持的模型: {model_name}。请添加自定义钩子路径。\")\n",
    "print(f\"模型总层数: {num_layers}\")\n",
    "\n",
    "# 循环每个数据集\n",
    "for ds_name, ds_config, ds_split, text_column in datasets_to_use:\n",
    "    print(f\"\\n=== 处理数据集: {ds_name} (split: {ds_split}) ===\")\n",
    "    try:\n",
    "        # 加载数据集\n",
    "        if ds_config:\n",
    "            dataset = load_dataset(ds_name, ds_config, split=ds_split)\n",
    "        else:\n",
    "            dataset = load_dataset(ds_name, split=ds_split)\n",
    "        \n",
    "        # 提取前num_samples个样本的文本\n",
    "        text_list = dataset[text_column][:num_samples]  # 获取文本列表\n",
    "        \n",
    "        # 特殊处理：如果数据集需要组合多个列（如squad）\n",
    "        if ds_name == \"squad\":\n",
    "            context_list = dataset['context'][:num_samples]\n",
    "            text_list = [q + \" \" + c for q, c in zip(text_list, context_list)]  # 组合question + context\n",
    "        \n",
    "        samples = [text[:max_length] for text in text_list if isinstance(text, str) and text.strip()]  # 截断、过滤无效\n",
    "        if len(samples) < num_samples:\n",
    "            print(f\"警告: 数据集只有 {len(samples)} 个有效样本\")\n",
    "        if len(samples) == 0:\n",
    "            print(\"跳过: 无有效样本\")\n",
    "            continue\n",
    "        print(f\"使用 {len(samples)} 个样本进行分析\")\n",
    "        \n",
    "        # 重置激活列表\n",
    "        activations_per_layer = [[] for _ in range(num_layers)]\n",
    "        \n",
    "        # 批量运行推理\n",
    "        with torch.no_grad():\n",
    "            for text in tqdm(samples, desc=\"处理样本\"):\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n",
    "                _ = model(**inputs)  # 运行推理，钩子捕获\n",
    "        \n",
    "        # 分析激活（与之前相同，处理变长序列）\n",
    "        if not any(activations_per_layer):\n",
    "            print(\"未捕获到激活值，跳过分析\")\n",
    "            continue\n",
    "        \n",
    "        # 计算整体平均激活密度\n",
    "        densities = []\n",
    "        for layer_acts in activations_per_layer:\n",
    "            for act in layer_acts:\n",
    "                if act.numel() > 0:\n",
    "                    density = (act.abs() > activation_threshold).float().mean().item()\n",
    "                    densities.append(density)\n",
    "        if densities:\n",
    "            avg_density = sum(densities) / len(densities)\n",
    "        else:\n",
    "            avg_density = 0.0\n",
    "            print(\"无有效激活数据，无法计算密度\")\n",
    "        \n",
    "        # 为每个层计算冷热神经元百分比，并收集数据（改进：基于激活频率）\n",
    "        layers_data = []\n",
    "        for layer_idx in range(num_layers):\n",
    "            layer_acts = activations_per_layer[layer_idx]\n",
    "            if not layer_acts:\n",
    "                continue\n",
    "            # 计算每个神经元的激活频率（跨样本平均比例）\n",
    "            neuron_freq = []  # 每个样本的频率向量列表\n",
    "            for act in layer_acts:\n",
    "                if act.numel() > 0:\n",
    "                    freq = (act.abs() > activation_threshold).float().mean(dim=[0, 1])  # 每个神经元的样本内平均激活比例\n",
    "                    neuron_freq.append(freq.unsqueeze(0))  # [1, hidden_size]\n",
    "            if not neuron_freq:\n",
    "                continue\n",
    "            # 跨样本平均频率 [hidden_size]\n",
    "            aggregated_freq = torch.cat(neuron_freq, dim=0).mean(dim=0)\n",
    "            total_neurons = aggregated_freq.size(0)\n",
    "            # 热神经元：平均频率 > hot_freq_threshold\n",
    "            hot_neurons = (aggregated_freq > hot_freq_threshold).sum().item()\n",
    "            hot_pct = (hot_neurons / total_neurons * 100) if total_neurons > 0 else 0\n",
    "            cold_pct = 100 - hot_pct\n",
    "            layers_data.append({\n",
    "                \"layer_id\": layer_idx,\n",
    "                \"total_neurons\": total_neurons,\n",
    "                \"hot_neurons\": hot_neurons,\n",
    "                \"hot_pct\": hot_pct,\n",
    "                \"cold_neurons\": total_neurons - hot_neurons,\n",
    "                \"cold_pct\": cold_pct\n",
    "            })\n",
    "        \n",
    "        # 保存为JSON：创建模型名称的文件夹（清理斜杠为下划线）\n",
    "        model_folder = model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")  # 清理为有效文件夹名，例如 \"meta_llama_Llama_3_2_1B_Instruct\"\n",
    "        os.makedirs(model_folder, exist_ok=True)  # 创建文件夹，如果不存在\n",
    "        json_filename = f\"{ds_name}_activation_results.json\"\n",
    "        json_path = os.path.join(model_folder, json_filename)  # 完整路径\n",
    "        results = {\n",
    "            \"dataset\": ds_name,\n",
    "            \"split\": ds_split,\n",
    "            \"num_samples\": len(samples),\n",
    "            \"average_density\": avg_density,\n",
    "            \"layers\": layers_data\n",
    "        }\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\"结果保存为: {json_path}\")\n",
    "        \n",
    "        # 清空GPU缓存，准备下一个数据集\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"处理 {ds_name} 失败: {e}\")\n",
    "\n",
    "# 移除钩子（在所有数据集后）\n",
    "for handle in handles:\n",
    "    handle.remove()\n",
    "print(\"\\n所有数据集处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892fe826",
   "metadata": {},
   "source": [
    "统计激活百分比、稀疏度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee643b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [01:57<00:00, 58.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型失败：CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 9.06 MiB is free. Including non-PyTorch memory, this process has 23.51 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 52.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "检测到的模型类型: phiforcausallm\n",
      "模型总层数: 32\n",
      "成功注册 32 个钩子\n",
      "\n",
      "=== 处理数据集: wikitext (split: test) ===\n",
      "使用 321 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 321/321 [00:07<00:00, 40.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wikitext 数据集结果:\n",
      "  - 整体平均激活百分比: 99.70%\n",
      "  - 各层激活百分比范围: 99.53% - 99.85%\n",
      "\n",
      "=== 处理数据集: gsm8k (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:12<00:00, 39.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gsm8k 数据集结果:\n",
      "  - 整体平均激活百分比: 99.71%\n",
      "  - 各层激活百分比范围: 99.64% - 99.84%\n",
      "\n",
      "=== 处理数据集: cc_news (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:12<00:00, 39.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cc_news 数据集结果:\n",
      "  - 整体平均激活百分比: 99.71%\n",
      "  - 各层激活百分比范围: 99.64% - 99.83%\n",
      "\n",
      "=== 处理数据集: squad (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:12<00:00, 40.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "squad 数据集结果:\n",
      "  - 整体平均激活百分比: 99.72%\n",
      "  - 各层激活百分比范围: 99.66% - 99.84%\n",
      "\n",
      "=== 处理数据集: cnn_dailymail (split: train) ===\n",
      "使用 500 个样本进行分析\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 500/500 [00:12<00:00, 39.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cnn_dailymail 数据集结果:\n",
      "  - 整体平均激活百分比: 99.71%\n",
      "  - 各层激活百分比范围: 99.64% - 99.83%\n",
      "\n",
      "综合结果保存为: microsoft_Phi_3.5_mini_instruct/activation_summary_all_datasets.json\n",
      "比较结果保存为: microsoft_Phi_3.5_mini_instruct/activation_comparison.json\n",
      "\n",
      "所有数据集处理完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Enable expandable segments to avoid fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 配置\n",
    "# model_name = \"microsoft/phi-2\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# model_name = \"Qwen/Qwen1.5-4B-Chat\"\n",
    "# model_name = \"THUDM/chatglm2-6b\"\n",
    "# model_name = \"facebook/opt-6.7b\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "num_samples = 500\n",
    "max_length = 128\n",
    "activation_threshold = 1e-3  # 激活阈值\n",
    "\n",
    "# 数据集列表\n",
    "datasets_to_use = [\n",
    "    (\"wikitext\", \"wikitext-2-raw-v1\", \"test\", \"text\"),\n",
    "    (\"gsm8k\", \"main\", \"train\", \"question\"),\n",
    "    (\"cc_news\", None, \"train\", \"text\"),\n",
    "    (\"squad\", None, \"train\", \"question\"),\n",
    "    (\"cnn_dailymail\", \"3.0.0\", \"train\", \"article\")\n",
    "]\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # 添加pad_token如果不存在\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    print(\"模型加载成功！\")\n",
    "    print(f\"模型类型: {type(model).__name__}\")\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"加载模型失败：{e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 定义钩子函数 - 修改为实时计算激活百分比\n",
    "activation_percentages_per_layer = []\n",
    "handles = []\n",
    "\n",
    "def hook_fn(layer_idx):\n",
    "    def fn(module, input, output):\n",
    "        with torch.no_grad():\n",
    "            if isinstance(output, tuple):\n",
    "                act = output[0]\n",
    "            else:\n",
    "                act = output\n",
    "            \n",
    "            # 计算当前批次的激活百分比\n",
    "            if act.dim() == 3:\n",
    "                # 对每个token计算激活神经元数，然后平均\n",
    "                activated = (act.abs() > activation_threshold).float()\n",
    "                # 沿着batch和sequence维度计算每个神经元的激活率\n",
    "                activation_rate = activated.mean(dim=[0, 1])  # [hidden_size]\n",
    "                # 计算激活神经元的百分比\n",
    "                activation_pct = activation_rate.mean().item() * 100\n",
    "            else:\n",
    "                # 处理其他维度的情况\n",
    "                activated = (act.abs() > activation_threshold).float()\n",
    "                activation_pct = activated.mean().item() * 100\n",
    "            \n",
    "            activation_percentages_per_layer[layer_idx].append(activation_pct)\n",
    "    return fn\n",
    "\n",
    "# 获取层数和注册钩子 - 改进模型类型检测\n",
    "model_type = type(model).__name__.lower()\n",
    "print(f\"检测到的模型类型: {model_type}\")\n",
    "\n",
    "# 根据模型类型选择正确的层路径\n",
    "if \"opt\" in model_type:\n",
    "    # OPT模型：model.model.decoder.layers\n",
    "    num_layers = len(model.model.decoder.layers)\n",
    "    for i in range(num_layers):\n",
    "        hook = model.model.decoder.layers[i].fc2.register_forward_hook(hook_fn(i))\n",
    "        handles.append(hook)\n",
    "elif \"chatglm\" in model_type:\n",
    "    # ChatGLM模型\n",
    "    num_layers = len(model.transformer.encoder.layers)\n",
    "    for i in range(num_layers):\n",
    "        hook = model.transformer.encoder.layers[i].mlp.register_forward_hook(hook_fn(i))\n",
    "        handles.append(hook)\n",
    "elif hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "    # 大多数现代模型（Qwen, LLaMA, Gemma, Mistral, Phi等）\n",
    "    num_layers = len(model.model.layers)\n",
    "    for i in range(num_layers):\n",
    "        # 检查MLP模块的具体结构\n",
    "        layer = model.model.layers[i]\n",
    "        if hasattr(layer, 'mlp'):\n",
    "            hook = layer.mlp.register_forward_hook(hook_fn(i))\n",
    "            handles.append(hook)\n",
    "        elif hasattr(layer, 'feed_forward'):\n",
    "            hook = layer.feed_forward.register_forward_hook(hook_fn(i))\n",
    "            handles.append(hook)\n",
    "        else:\n",
    "            print(f\"警告: 第{i}层未找到MLP模块\")\n",
    "elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "    # GPT-2风格的模型\n",
    "    num_layers = len(model.transformer.h)\n",
    "    for i in range(num_layers):\n",
    "        hook = model.transformer.h[i].mlp.register_forward_hook(hook_fn(i))\n",
    "        handles.append(hook)\n",
    "else:\n",
    "    # 尝试自动检测层结构\n",
    "    print(\"尝试自动检测模型结构...\")\n",
    "    # 打印模型结构以便调试\n",
    "    for name, module in model.named_modules():\n",
    "        if 'layer' in name.lower() or 'block' in name.lower():\n",
    "            print(f\"发现层: {name}\")\n",
    "    raise ValueError(f\"无法自动检测模型 {model_name} 的层结构。请手动添加支持。\")\n",
    "\n",
    "print(f\"模型总层数: {num_layers}\")\n",
    "print(f\"成功注册 {len(handles)} 个钩子\")\n",
    "\n",
    "# 存储所有数据集的结果\n",
    "all_dataset_results = {}\n",
    "\n",
    "# 循环每个数据集\n",
    "for ds_name, ds_config, ds_split, text_column in datasets_to_use:\n",
    "    print(f\"\\n=== 处理数据集: {ds_name} (split: {ds_split}) ===\")\n",
    "    try:\n",
    "        # 加载数据集\n",
    "        if ds_config:\n",
    "            dataset = load_dataset(ds_name, ds_config, split=ds_split)\n",
    "        else:\n",
    "            dataset = load_dataset(ds_name, split=ds_split)\n",
    "        \n",
    "        # 提取样本文本\n",
    "        text_list = dataset[text_column][:num_samples]\n",
    "        \n",
    "        # 特殊处理squad数据集\n",
    "        if ds_name == \"squad\":\n",
    "            context_list = dataset['context'][:num_samples]\n",
    "            text_list = [q + \" \" + c for q, c in zip(text_list, context_list)]\n",
    "        \n",
    "        samples = [text for text in text_list if isinstance(text, str) and text.strip()]\n",
    "        if len(samples) == 0:\n",
    "            print(\"跳过: 无有效样本\")\n",
    "            continue\n",
    "        print(f\"使用 {len(samples)} 个样本进行分析\")\n",
    "        \n",
    "        # 重置每层的激活百分比列表\n",
    "        activation_percentages_per_layer = [[] for _ in range(num_layers)]\n",
    "        \n",
    "        # 批量运行推理\n",
    "        with torch.no_grad():\n",
    "            for text in tqdm(samples, desc=\"处理样本\"):\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                # 截断文本\n",
    "                text = text[:max_length]\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, \n",
    "                                 truncation=True, padding=True).to(device)\n",
    "                try:\n",
    "                    _ = model(**inputs)\n",
    "                except Exception as e:\n",
    "                    print(f\"推理错误: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # 计算每层的平均激活百分比\n",
    "        layer_avg_activations = []\n",
    "        for layer_idx in range(num_layers):\n",
    "            layer_percentages = activation_percentages_per_layer[layer_idx]\n",
    "            if layer_percentages:\n",
    "                avg_pct = np.mean(layer_percentages)\n",
    "                layer_avg_activations.append({\n",
    "                    \"layer_id\": layer_idx,\n",
    "                    \"avg_activation_percentage\": round(avg_pct, 4),\n",
    "                    \"num_samples\": len(layer_percentages)\n",
    "                })\n",
    "            else:\n",
    "                layer_avg_activations.append({\n",
    "                    \"layer_id\": layer_idx,\n",
    "                    \"avg_activation_percentage\": 0.0,\n",
    "                    \"num_samples\": 0\n",
    "                })\n",
    "        \n",
    "        # 计算所有层的整体平均激活百分比\n",
    "        all_layer_percentages = []\n",
    "        for layer_data in layer_avg_activations:\n",
    "            if layer_data[\"num_samples\"] > 0:\n",
    "                all_layer_percentages.append(layer_data[\"avg_activation_percentage\"])\n",
    "        \n",
    "        overall_avg = np.mean(all_layer_percentages) if all_layer_percentages else 0.0\n",
    "        \n",
    "        # 保存该数据集的结果\n",
    "        dataset_result = {\n",
    "            \"dataset\": ds_name,\n",
    "            \"split\": ds_split,\n",
    "            \"num_samples\": len(samples),\n",
    "            \"activation_threshold\": activation_threshold,\n",
    "            \"overall_avg_activation_percentage\": round(overall_avg, 4),\n",
    "            \"layers\": layer_avg_activations\n",
    "        }\n",
    "        \n",
    "        all_dataset_results[ds_name] = dataset_result\n",
    "        \n",
    "        # 打印摘要\n",
    "        print(f\"\\n{ds_name} 数据集结果:\")\n",
    "        print(f\"  - 整体平均激活百分比: {overall_avg:.2f}%\")\n",
    "        if all_layer_percentages:\n",
    "            print(f\"  - 各层激活百分比范围: {min(all_layer_percentages):.2f}% - {max(all_layer_percentages):.2f}%\")\n",
    "        \n",
    "        # 清空GPU缓存\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"处理 {ds_name} 失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 保存所有结果到一个综合JSON文件\n",
    "model_folder = model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# 保存各数据集的详细结果\n",
    "summary_filename = \"activation_summary_all_datasets.json\"\n",
    "summary_path = os.path.join(model_folder, summary_filename)\n",
    "summary_results = {\n",
    "    \"model\": model_name,\n",
    "    \"model_type\": model_type,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"activation_threshold\": activation_threshold,\n",
    "    \"datasets\": all_dataset_results\n",
    "}\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_results, f, indent=4)\n",
    "print(f\"\\n综合结果保存为: {summary_path}\")\n",
    "\n",
    "# 生成简化的比较表格\n",
    "comparison_filename = \"activation_comparison.json\"\n",
    "comparison_path = os.path.join(model_folder, comparison_filename)\n",
    "comparison_data = {\n",
    "    \"model\": model_name,\n",
    "    \"activation_threshold\": activation_threshold,\n",
    "    \"dataset_averages\": {}\n",
    "}\n",
    "for ds_name, result in all_dataset_results.items():\n",
    "    comparison_data[\"dataset_averages\"][ds_name] = {\n",
    "        \"overall_activation_percentage\": result[\"overall_avg_activation_percentage\"],\n",
    "        \"num_samples\": result[\"num_samples\"]\n",
    "    }\n",
    "with open(comparison_path, 'w') as f:\n",
    "    json.dump(comparison_data, f, indent=4)\n",
    "print(f\"比较结果保存为: {comparison_path}\")\n",
    "\n",
    "# 移除钩子\n",
    "for handle in handles:\n",
    "    handle.remove()\n",
    "print(\"\\n所有数据集处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e2c9a",
   "metadata": {},
   "source": [
    "添加循环，顺序执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a79017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable expandable segments to avoid fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 配置\n",
    "# 定义所有要测试的模型列表\n",
    "MODEL_LIST = [\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"Qwen/Qwen1.5-1.8B-Chat\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"microsoft/phi-2\",\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    \"Qwen/Qwen1.5-4B-Chat\",\n",
    "    \"THUDM/chatglm2-6b\",\n",
    "    \"facebook/opt-6.7b\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "]\n",
    "\n",
    "# 通用配置\n",
    "num_samples = 500\n",
    "max_length = 128\n",
    "activation_threshold = 1e-3\n",
    "batch_size = 8\n",
    "\n",
    "# 数据集列表\n",
    "datasets_to_use = [\n",
    "    (\"wikitext\", \"wikitext-2-raw-v1\", \"test\", \"text\"),\n",
    "    (\"gsm8k\", \"main\", \"train\", \"question\"),\n",
    "    (\"cc_news\", None, \"train\", \"text\"),\n",
    "    (\"squad\", None, \"train\", \"question\"),\n",
    "    (\"cnn_dailymail\", \"3.0.0\", \"train\", \"article\")\n",
    "]\n",
    "\n",
    "# 定义清理显存的函数\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"清理GPU显存\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "# 定义处理单个模型的函数\n",
    "def process_single_model(model_name):\n",
    "    \"\"\"处理单个模型的所有数据集测试\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"开始处理模型: {model_name}\")\n",
    "    print(f\"时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # 记录开始时间\n",
    "    model_start_time = time.time()\n",
    "    \n",
    "    # 加载模型和tokenizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # 加载tokenizer\n",
    "        print(\"加载tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # 加载模型\n",
    "        print(\"加载模型...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,  # 使用半精度以节省显存\n",
    "            device_map=\"auto\"  # 自动设备映射\n",
    "        )\n",
    "        model.eval()\n",
    "        print(\"模型加载成功！\")\n",
    "        print(f\"模型类型: {type(model).__name__}\")\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"加载模型失败：{e}\")\n",
    "        clear_gpu_memory()\n",
    "        return False\n",
    "    \n",
    "    # 定义钩子函数\n",
    "    activation_percentages_per_layer = []\n",
    "    handles = []\n",
    "    \n",
    "    def hook_fn(layer_idx):\n",
    "        def fn(module, input, output):\n",
    "            with torch.no_grad():\n",
    "                if isinstance(output, tuple):\n",
    "                    act = output[0]\n",
    "                else:\n",
    "                    act = output\n",
    "                \n",
    "                # 计算激活百分比\n",
    "                if act.dim() == 3:\n",
    "                    activated = (act.abs() > activation_threshold).float()\n",
    "                    activation_rate = activated.mean(dim=[0, 1])\n",
    "                    activation_pct = activation_rate.mean().item() * 100\n",
    "                else:\n",
    "                    activated = (act.abs() > activation_threshold).float()\n",
    "                    activation_pct = activated.mean().item() * 100\n",
    "                \n",
    "                activation_percentages_per_layer[layer_idx].append(activation_pct)\n",
    "                \n",
    "                # 清理内存\n",
    "                del act, activated\n",
    "                if 'activation_rate' in locals():\n",
    "                    del activation_rate\n",
    "        return fn\n",
    "    \n",
    "    # 注册钩子\n",
    "    try:\n",
    "        model_type = type(model).__name__.lower()\n",
    "        print(f\"检测到的模型类型: {model_type}\")\n",
    "        \n",
    "        # 根据模型类型选择正确的层路径\n",
    "        if \"opt\" in model_type:\n",
    "            num_layers = len(model.model.decoder.layers)\n",
    "            for i in range(num_layers):\n",
    "                hook = model.model.decoder.layers[i].fc2.register_forward_hook(hook_fn(i))\n",
    "                handles.append(hook)\n",
    "        elif \"chatglm\" in model_type:\n",
    "            num_layers = len(model.transformer.encoder.layers)\n",
    "            for i in range(num_layers):\n",
    "                hook = model.transformer.encoder.layers[i].mlp.register_forward_hook(hook_fn(i))\n",
    "                handles.append(hook)\n",
    "        elif hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "            num_layers = len(model.model.layers)\n",
    "            for i in range(num_layers):\n",
    "                layer = model.model.layers[i]\n",
    "                if hasattr(layer, 'mlp'):\n",
    "                    hook = layer.mlp.register_forward_hook(hook_fn(i))\n",
    "                    handles.append(hook)\n",
    "                elif hasattr(layer, 'feed_forward'):\n",
    "                    hook = layer.feed_forward.register_forward_hook(hook_fn(i))\n",
    "                    handles.append(hook)\n",
    "        elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "            num_layers = len(model.transformer.h)\n",
    "            for i in range(num_layers):\n",
    "                hook = model.transformer.h[i].mlp.register_forward_hook(hook_fn(i))\n",
    "                handles.append(hook)\n",
    "        else:\n",
    "            print(f\"无法自动检测模型 {model_name} 的层结构\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"模型总层数: {num_layers}\")\n",
    "        print(f\"成功注册 {len(handles)} 个钩子\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"注册钩子失败: {e}\")\n",
    "        # 清理已加载的模型\n",
    "        del model\n",
    "        clear_gpu_memory()\n",
    "        return False\n",
    "    \n",
    "    # 存储所有数据集的结果\n",
    "    all_dataset_results = {}\n",
    "    \n",
    "    # 循环每个数据集\n",
    "    for ds_name, ds_config, ds_split, text_column in datasets_to_use:\n",
    "        print(f\"\\n--- 处理数据集: {ds_name} (split: {ds_split}) ---\")\n",
    "        dataset_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 加载数据集\n",
    "            if ds_config:\n",
    "                dataset = load_dataset(ds_name, ds_config, split=ds_split)\n",
    "            else:\n",
    "                dataset = load_dataset(ds_name, split=ds_split)\n",
    "            \n",
    "            # 提取样本文本\n",
    "            text_list = dataset[text_column][:num_samples]\n",
    "            \n",
    "            # 特殊处理squad数据集\n",
    "            if ds_name == \"squad\":\n",
    "                context_list = dataset['context'][:num_samples]\n",
    "                text_list = [q + \" \" + c for q, c in zip(text_list, context_list)]\n",
    "            \n",
    "            samples = [text for text in text_list if isinstance(text, str) and text.strip()]\n",
    "            if len(samples) == 0:\n",
    "                print(\"跳过: 无有效样本\")\n",
    "                continue\n",
    "            print(f\"使用 {len(samples)} 个样本进行分析\")\n",
    "            \n",
    "            # 重置激活记录\n",
    "            activation_percentages_per_layer = [[] for _ in range(num_layers)]\n",
    "            \n",
    "            # 批量运行推理\n",
    "            with torch.no_grad():\n",
    "                for i, text in enumerate(tqdm(samples, desc=\"处理样本\")):\n",
    "                    if not text.strip():\n",
    "                        continue\n",
    "                    text = text[:max_length]\n",
    "                    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, \n",
    "                                     truncation=True, padding=True).to(device)\n",
    "                    try:\n",
    "                        outputs = model(**inputs)\n",
    "                        del outputs\n",
    "                    except Exception as e:\n",
    "                        print(f\"推理错误: {e}\")\n",
    "                        continue\n",
    "                    finally:\n",
    "                        del inputs\n",
    "                    \n",
    "                    # 定期清理显存\n",
    "                    if (i + 1) % batch_size == 0:\n",
    "                        clear_gpu_memory()\n",
    "            \n",
    "            # 清理显存\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            # 计算统计数据\n",
    "            layer_avg_activations = []\n",
    "            for layer_idx in range(num_layers):\n",
    "                layer_percentages = activation_percentages_per_layer[layer_idx]\n",
    "                if layer_percentages:\n",
    "                    avg_pct = np.mean(layer_percentages)\n",
    "                    layer_avg_activations.append({\n",
    "                        \"layer_id\": layer_idx,\n",
    "                        \"avg_activation_percentage\": round(avg_pct, 4),\n",
    "                        \"num_samples\": len(layer_percentages)\n",
    "                    })\n",
    "                else:\n",
    "                    layer_avg_activations.append({\n",
    "                        \"layer_id\": layer_idx,\n",
    "                        \"avg_activation_percentage\": 0.0,\n",
    "                        \"num_samples\": 0\n",
    "                    })\n",
    "            \n",
    "            # 计算整体平均\n",
    "            all_layer_percentages = [\n",
    "                layer_data[\"avg_activation_percentage\"] \n",
    "                for layer_data in layer_avg_activations \n",
    "                if layer_data[\"num_samples\"] > 0\n",
    "            ]\n",
    "            overall_avg = np.mean(all_layer_percentages) if all_layer_percentages else 0.0\n",
    "            \n",
    "            # 保存结果\n",
    "            dataset_result = {\n",
    "                \"dataset\": ds_name,\n",
    "                \"split\": ds_split,\n",
    "                \"num_samples\": len(samples),\n",
    "                \"activation_threshold\": activation_threshold,\n",
    "                \"overall_avg_activation_percentage\": round(overall_avg, 4),\n",
    "                \"layers\": layer_avg_activations,\n",
    "                \"processing_time\": round(time.time() - dataset_start_time, 2)\n",
    "            }\n",
    "            \n",
    "            all_dataset_results[ds_name] = dataset_result\n",
    "            \n",
    "            # 打印摘要\n",
    "            print(f\"\\n{ds_name} 数据集结果:\")\n",
    "            print(f\"  - 整体平均激活百分比: {overall_avg:.2f}%\")\n",
    "            print(f\"  - 处理时间: {dataset_result['processing_time']} 秒\")\n",
    "            \n",
    "            # 清理变量\n",
    "            del dataset, text_list, samples\n",
    "            if ds_name == \"squad\" and 'context_list' in locals():\n",
    "                del context_list\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理 {ds_name} 失败: {e}\")\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    # 保存结果\n",
    "    model_folder = model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    \n",
    "    # 详细结果\n",
    "    summary_filename = \"activation_summary_all_datasets.json\"\n",
    "    summary_path = os.path.join(model_folder, summary_filename)\n",
    "    summary_results = {\n",
    "        \"model\": model_name,\n",
    "        \"model_type\": model_type,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"activation_threshold\": activation_threshold,\n",
    "        \"total_processing_time\": round(time.time() - model_start_time, 2),\n",
    "        \"datasets\": all_dataset_results\n",
    "    }\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_results, f, indent=4)\n",
    "    print(f\"\\n综合结果保存为: {summary_path}\")\n",
    "    \n",
    "    # 简化比较结果\n",
    "    comparison_filename = \"activation_comparison.json\"\n",
    "    comparison_path = os.path.join(model_folder, comparison_filename)\n",
    "    comparison_data = {\n",
    "        \"model\": model_name,\n",
    "        \"activation_threshold\": activation_threshold,\n",
    "        \"dataset_averages\": {}\n",
    "    }\n",
    "    for ds_name, result in all_dataset_results.items():\n",
    "        comparison_data[\"dataset_averages\"][ds_name] = {\n",
    "            \"overall_activation_percentage\": result[\"overall_avg_activation_percentage\"],\n",
    "            \"num_samples\": result[\"num_samples\"]\n",
    "        }\n",
    "    with open(comparison_path, 'w') as f:\n",
    "        json.dump(comparison_data, f, indent=4)\n",
    "    \n",
    "    # 移除钩子\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    # 删除模型并清理显存\n",
    "    del model\n",
    "    if 'tokenizer' in locals():\n",
    "        del tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    print(f\"\\n模型 {model_name} 处理完成！\")\n",
    "    print(f\"总处理时间: {round(time.time() - model_start_time, 2)} 秒\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 主程序：循环处理所有模型\n",
    "def main():\n",
    "    \"\"\"主函数：循环处理所有模型\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"批量模型激活稀疏度测试\")\n",
    "    print(f\"开始时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"待测试模型数量: {len(MODEL_LIST)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 创建总结果文件夹\n",
    "    os.makedirs(\"all_models_results\", exist_ok=True)\n",
    "    \n",
    "    # 记录处理状态\n",
    "    processing_log = {\n",
    "        \"start_time\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"models\": {}\n",
    "    }\n",
    "    \n",
    "    successful_models = []\n",
    "    failed_models = []\n",
    "    \n",
    "    # 循环处理每个模型\n",
    "    for idx, model_name in enumerate(MODEL_LIST, 1):\n",
    "        print(f\"\\n\\n{'#'*80}\")\n",
    "        print(f\"进度: {idx}/{len(MODEL_LIST)}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        try:\n",
    "            success = process_single_model(model_name)\n",
    "            if success:\n",
    "                successful_models.append(model_name)\n",
    "                processing_log[\"models\"][model_name] = \"成功\"\n",
    "            else:\n",
    "                failed_models.append(model_name)\n",
    "                processing_log[\"models\"][model_name] = \"失败\"\n",
    "        except Exception as e:\n",
    "            print(f\"处理模型 {model_name} 时发生严重错误: {e}\")\n",
    "            failed_models.append(model_name)\n",
    "            processing_log[\"models\"][model_name] = f\"错误: {str(e)}\"\n",
    "        \n",
    "        # 每个模型处理完后都清理显存\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        # 短暂等待，确保资源完全释放\n",
    "        time.sleep(5)\n",
    "    \n",
    "    # 保存处理日志\n",
    "    processing_log[\"end_time\"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    processing_log[\"successful_models\"] = successful_models\n",
    "    processing_log[\"failed_models\"] = failed_models\n",
    "    processing_log[\"success_rate\"] = f\"{len(successful_models)}/{len(MODEL_LIST)}\"\n",
    "    \n",
    "    log_path = os.path.join(\"all_models_results\", \"processing_log.json\")\n",
    "    with open(log_path, 'w') as f:\n",
    "        json.dump(processing_log, f, indent=4)\n",
    "    \n",
    "    # 打印最终统计\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"所有模型处理完成！\")\n",
    "    print(f\"结束时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"成功: {len(successful_models)} 个模型\")\n",
    "    print(f\"失败: {len(failed_models)} 个模型\")\n",
    "    \n",
    "    if successful_models:\n",
    "        print(\"\\n成功的模型:\")\n",
    "        for model in successful_models:\n",
    "            print(f\"  ✓ {model}\")\n",
    "    \n",
    "    if failed_models:\n",
    "        print(\"\\n失败的模型:\")\n",
    "        for model in failed_models:\n",
    "            print(f\"  ✗ {model}\")\n",
    "    \n",
    "    print(f\"\\n处理日志保存在: {log_path}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# 运行主程序\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
